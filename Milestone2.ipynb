{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRA Tweets: Analysis on Italian Tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import networkx as nx \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Reading and Exploration\n",
    "\n",
    "### Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In reading phase we read the input data (both zip files), we extract italian tweets and we preprocess it. Finally we store the result in two new csv files to an easy future access. We consider also another csv file with without italian tweets with region column equal to USA, since we have noticed that are difficult to interpret. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we iterate through \"russian-troll-tweets.zip\" to read through every .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first IRA dataset\n",
    "data_folder = './data/'\n",
    "zip_file = ZipFile(data_folder+'russian-troll-tweets.zip')\n",
    "fs = {text_file.filename: pd.read_csv(zip_file.open(text_file.filename))\n",
    "       for text_file in zip_file.infolist()\n",
    "       if text_file.filename.endswith('.csv')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_author_id</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>region</th>\n",
       "      <th>language</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>harvested_date</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>updates</th>\n",
       "      <th>post_type</th>\n",
       "      <th>account_type</th>\n",
       "      <th>new_june_2018</th>\n",
       "      <th>retweet</th>\n",
       "      <th>account_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.060000e+17</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>\"We have a sitting Democrat US Senator on tria...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 19:58</td>\n",
       "      <td>10/1/2017 19:59</td>\n",
       "      <td>1052</td>\n",
       "      <td>9636</td>\n",
       "      <td>253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.060000e+17</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Marshawn Lynch arrives to game in anti-Trump s...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 22:43</td>\n",
       "      <td>10/1/2017 22:43</td>\n",
       "      <td>1054</td>\n",
       "      <td>9637</td>\n",
       "      <td>254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.060000e+17</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Daughter of fallen Navy Sailor delivers powerf...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 22:50</td>\n",
       "      <td>10/1/2017 22:51</td>\n",
       "      <td>1054</td>\n",
       "      <td>9637</td>\n",
       "      <td>255</td>\n",
       "      <td>RETWEET</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.060000e+17</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>JUST IN: President Trump dedicates Presidents ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 23:52</td>\n",
       "      <td>10/1/2017 23:52</td>\n",
       "      <td>1062</td>\n",
       "      <td>9642</td>\n",
       "      <td>256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.060000e+17</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>19,000 RESPECTING our National Anthem! #StandF...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 2:13</td>\n",
       "      <td>10/1/2017 2:13</td>\n",
       "      <td>1050</td>\n",
       "      <td>9645</td>\n",
       "      <td>246</td>\n",
       "      <td>RETWEET</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   external_author_id  author  \\\n",
       "0        9.060000e+17  10_GOP   \n",
       "1        9.060000e+17  10_GOP   \n",
       "2        9.060000e+17  10_GOP   \n",
       "3        9.060000e+17  10_GOP   \n",
       "4        9.060000e+17  10_GOP   \n",
       "\n",
       "                                             content   region language  \\\n",
       "0  \"We have a sitting Democrat US Senator on tria...  Unknown  English   \n",
       "1  Marshawn Lynch arrives to game in anti-Trump s...  Unknown  English   \n",
       "2  Daughter of fallen Navy Sailor delivers powerf...  Unknown  English   \n",
       "3  JUST IN: President Trump dedicates Presidents ...  Unknown  English   \n",
       "4  19,000 RESPECTING our National Anthem! #StandF...  Unknown  English   \n",
       "\n",
       "      publish_date   harvested_date  following  followers  updates post_type  \\\n",
       "0  10/1/2017 19:58  10/1/2017 19:59       1052       9636      253       NaN   \n",
       "1  10/1/2017 22:43  10/1/2017 22:43       1054       9637      254       NaN   \n",
       "2  10/1/2017 22:50  10/1/2017 22:51       1054       9637      255   RETWEET   \n",
       "3  10/1/2017 23:52  10/1/2017 23:52       1062       9642      256       NaN   \n",
       "4   10/1/2017 2:13   10/1/2017 2:13       1050       9645      246   RETWEET   \n",
       "\n",
       "  account_type  new_june_2018  retweet account_category  \n",
       "0        Right              0        0       RightTroll  \n",
       "1        Right              0        0       RightTroll  \n",
       "2        Right              0        1       RightTroll  \n",
       "3        Right              0        0       RightTroll  \n",
       "4        Right              0        1       RightTroll  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs['IRAhandle_tweets_1.csv'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We append all datasets together in one DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame()\n",
    "for key,val in fs.items():\n",
    "    data=pd.concat([data,val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_author_id</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>updates</th>\n",
       "      <th>new_june_2018</th>\n",
       "      <th>retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.973367e+06</td>\n",
       "      <td>2.973371e+06</td>\n",
       "      <td>2.973371e+06</td>\n",
       "      <td>2.973371e+06</td>\n",
       "      <td>2.973371e+06</td>\n",
       "      <td>2.973371e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.296128e+17</td>\n",
       "      <td>3.433524e+03</td>\n",
       "      <td>7.018913e+03</td>\n",
       "      <td>1.049756e+04</td>\n",
       "      <td>2.078735e-01</td>\n",
       "      <td>4.408955e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.036341e+17</td>\n",
       "      <td>5.609881e+03</td>\n",
       "      <td>1.458463e+04</td>\n",
       "      <td>1.768729e+04</td>\n",
       "      <td>4.057859e-01</td>\n",
       "      <td>4.964945e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.497640e+07</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.930748e+09</td>\n",
       "      <td>3.270000e+02</td>\n",
       "      <td>3.200000e+02</td>\n",
       "      <td>1.787000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.581835e+09</td>\n",
       "      <td>1.499000e+03</td>\n",
       "      <td>1.274000e+03</td>\n",
       "      <td>4.333000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.254274e+09</td>\n",
       "      <td>4.730000e+03</td>\n",
       "      <td>1.060000e+04</td>\n",
       "      <td>1.234100e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.812510e+17</td>\n",
       "      <td>7.621000e+04</td>\n",
       "      <td>2.512760e+05</td>\n",
       "      <td>1.661130e+05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       external_author_id     following     followers       updates  \\\n",
       "count        2.973367e+06  2.973371e+06  2.973371e+06  2.973371e+06   \n",
       "mean         1.296128e+17  3.433524e+03  7.018913e+03  1.049756e+04   \n",
       "std          3.036341e+17  5.609881e+03  1.458463e+04  1.768729e+04   \n",
       "min          3.497640e+07 -1.000000e+00 -1.000000e+00 -1.000000e+00   \n",
       "25%          1.930748e+09  3.270000e+02  3.200000e+02  1.787000e+03   \n",
       "50%          2.581835e+09  1.499000e+03  1.274000e+03  4.333000e+03   \n",
       "75%          3.254274e+09  4.730000e+03  1.060000e+04  1.234100e+04   \n",
       "max          9.812510e+17  7.621000e+04  2.512760e+05  1.661130e+05   \n",
       "\n",
       "       new_june_2018       retweet  \n",
       "count   2.973371e+06  2.973371e+06  \n",
       "mean    2.078735e-01  4.408955e-01  \n",
       "std     4.057859e-01  4.964945e-01  \n",
       "min     0.000000e+00  0.000000e+00  \n",
       "25%     0.000000e+00  0.000000e+00  \n",
       "50%     0.000000e+00  0.000000e+00  \n",
       "75%     0.000000e+00  1.000000e+00  \n",
       "max     1.000000e+00  1.000000e+00  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we preprocess the data, we notice that *content*, the main attribute of our analysis, has some null values. We prefer removing them. Finally we remove duplicate values of *content*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "external_author_id     True\n",
       "author                False\n",
       "content                True\n",
       "region                 True\n",
       "language              False\n",
       "publish_date          False\n",
       "harvested_date        False\n",
       "following             False\n",
       "followers             False\n",
       "updates               False\n",
       "post_type              True\n",
       "account_type           True\n",
       "new_june_2018         False\n",
       "retweet               False\n",
       "account_category      False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset= ['content'])\n",
    "data.content.is_unique # Answer : False\n",
    "data = data.drop_duplicates(subset = 'content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select only the italian tweets from the first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Italian tweets\n",
    "italian_tweets = data[data.language == 'Italian']\n",
    "\n",
    "# Italian tweets and not USA region\n",
    "italian_tweets_notusa = italian_tweets[italian_tweets.region != 'United States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tweets.to_csv(\"italian_tweets_old.csv\")\n",
    "italian_tweets_notusa.to_csv(\"italian_tweets_old_notusa.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also import from a second dataset('ira_tweets_new.zip') and we repeat the previous analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Read the second IRA dataset\n",
    "zip_file = ZipFile(data_folder+'ira_tweets_new.zip')\n",
    "interesting = ['rus_troll_tweet_text.csv',\n",
    "               'rus_troll_tweet_stats.csv',\n",
    "               'rus_troll_tweet_metadata.csv']\n",
    "fs = {text_file.filename: pd.read_csv(zip_file.open(text_file.filename))\n",
    "       for text_file in zip_file.infolist()\n",
    "       if text_file.filename in interesting}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanew=pd.DataFrame()\n",
    "for key,val in fs.items():\n",
    "    datanew=pd.concat([datanew,val], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweetid                 False\n",
       "userid                  False\n",
       "tweet_time              False\n",
       "in_reply_to_tweetid      True\n",
       "in_reply_to_userid       True\n",
       "quoted_tweet_tweetid     True\n",
       "is_retweet              False\n",
       "retweet_userid           True\n",
       "retweet_tweetid          True\n",
       "quote_count             False\n",
       "reply_count             False\n",
       "like_count              False\n",
       "retweet_count           False\n",
       "hashtags                 True\n",
       "urls                     True\n",
       "user_mentions            True\n",
       "poll_choices             True\n",
       "tweetid                 False\n",
       "tweet_language           True\n",
       "tweet_text               True\n",
       "tweetid                 False\n",
       "follower_count          False\n",
       "following_count         False\n",
       "latitude                 True\n",
       "longitude                True\n",
       "tweet_client_name        True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datanew.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datanew = datanew.dropna(subset= ['tweet_text'])\n",
    "datanew.tweet_text.is_unique # Answer : False\n",
    "datanew = datanew.drop_duplicates(subset = 'tweet_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.read_csv(zip_file.open('rus_troll_user.csv'))\n",
    "italian_tweets = datanew[datanew.tweet_language == 'it']\n",
    "italian_tweets = pd.merge(italian_tweets, data4, on ='userid')\n",
    "italian_tweets.shape\n",
    "italian_tweets.to_csv(\"italian_tweets.csv\")\n",
    "italian_tweets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors: following and followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tweets = pd.read_csv('italian_tweets.csv')\n",
    "italian_tweets_old = pd.read_csv('italian_tweets_old.csv')\n",
    "italian_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tweets_old.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#italian_tweets['tweet_time'] =  pd.to_datetime(italian_tweets['tweet_time'], format='%Y/%m/%d %H:%M')\n",
    "#italian_tweets_old['publish_date'] =  pd.to_datetime(italian_tweets_old['publish_date'], format='%m/%d/%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tweets['tweet_time'] =  pd.to_datetime(italian_tweets['tweet_time'], format='%Y/%m/%d %H:%M')\n",
    "italian_tweets_old['publish_date'] =  pd.to_datetime(italian_tweets_old['publish_date'], format='%m/%d/%Y %H:%M')\n",
    "italian_tweets=italian_tweets.rename(index=str, columns={\"user_display_name\":\"author\",\n",
    "                                                         \"follower_count_x\": \"followers\",\n",
    "                                                         \"following_count_x\": \"following\",\n",
    "                                                         \"tweet_time\":\"time\"})\n",
    "italian_tweets_old=italian_tweets_old.rename(index=str, columns={\"publish_date\":\"time\"})\n",
    "italian_tweets=italian_tweets.rename(index=str, columns={\"tweet_text\":\"content\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers=pd.concat([italian_tweets[['author','followers','time']],italian_tweets_old[['author','followers','time']]])\n",
    "followers=followers.groupby('author').agg({'followers': 'max'}).rename(columns={'followers':'max_followers'})\n",
    "\n",
    "following=pd.concat([italian_tweets[['author','following','time']],italian_tweets_old[['author','following','time']]])\n",
    "following=following.groupby('author').agg({'following': 'max'}).rename(columns={'following':'max_following'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgfollowers=pd.concat([italian_tweets[['author','followers','time']],italian_tweets_old[['author','followers','time']]])\n",
    "avgfollowers=avgfollowers.groupby('author').agg({'followers': 'mean'}).rename(columns={'followers':'mean_followers'})\n",
    "\n",
    "avgfollowing=pd.concat([italian_tweets[['author','following','time']],italian_tweets_old[['author','following','time']]])\n",
    "avgfollowing=avgfollowing.groupby('author').agg({'following': 'mean'}).rename(columns={'following':'mean_following'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(20,13))\n",
    "fig.suptitle('Visualization of the distribution of following/followers', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax = fig.add_subplot(221)\n",
    "ax.hist(following.max_following,bins=100,log=True)\n",
    "ax.set_xlabel('Number of Following(Max)')\n",
    "ax.set_ylabel('Counts')\n",
    "ax.set_title(r'Max-Following Number Distribution for Each Author')\n",
    "plt.locator_params(axis='x',nbins=6)\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2.hist(followers.max_followers,bins=100,log=True)\n",
    "ax2.set_xlabel('Number of Followers(Max)')\n",
    "ax2.set_ylabel('Counts')\n",
    "ax2.set_title(r'Max-Followers Number Distribution for Each Author')\n",
    "plt.locator_params(axis='x',nbins=6)\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3.hist(avgfollowing.mean_following,bins=100,log=True)\n",
    "ax3.set_xlabel('Number of Mean-Following(Mean)')\n",
    "ax3.set_ylabel('Counts')\n",
    "ax3.set_title(r'Mean-Following Number Distribution for Each Author')\n",
    "plt.locator_params(axis='x',nbins=6)\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4.hist(avgfollowers.mean_followers,bins=100,log=True)\n",
    "ax4.set_xlabel('Number of Mean-Followers(Mean)')\n",
    "ax4.set_ylabel('Counts')\n",
    "ax4.set_title(r'Mean-Followers Number Distribution for Each Author')\n",
    "plt.locator_params(axis='x',nbins=6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters=italian_tweets.groupby('author').count()\n",
    "less_following = following.loc[following['max_following'] < 50000]\n",
    "less_followers = followers.loc[followers['max_followers'] < 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,4))\n",
    "fig.suptitle('Visualization of the distribution of following/followers(Number<50000)', fontsize=14, fontweight='bold')\n",
    "ax = fig.add_subplot(121)\n",
    "\n",
    "ax.hist(less_following.max_following,bins=100, log=True)\n",
    "ax.set_xlabel('Number of following')\n",
    "ax.set_ylabel('Counts')\n",
    "ax.set_title(r'Following')\n",
    "plt.locator_params(axis='x',nbins=4)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.hist(less_followers.max_followers,bins=100,log=True)\n",
    "ax2.set_xlabel('Number of followers')\n",
    "ax2.set_ylabel('Counts')\n",
    "ax2.set_title(r'Followers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors: activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tweets=italian_tweets.rename(index=str, columns={\"tweet_text\":\"content\"})\n",
    "authors=pd.concat([italian_tweets[['content','author','time']],italian_tweets_old[['content','author','time']]])\n",
    "authors=authors.drop_duplicates(subset='content')\n",
    "authors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we count the total number of all contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.content.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find out that every content is unique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(authors.content.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors['time']=authors['time'].apply(lambda x: x.date())#Only keep the date\n",
    "#Count total number of tweets the author sent out in one day:\n",
    "authors=authors.groupby(['author','time']).agg('count').rename(columns={'content':'tot'})\n",
    "#Take the average number of tweets one author sent out per day:\n",
    "authors=authors.groupby(by='author',as_index=True).agg({'tot':'mean'}).reset_index().rename(columns={'tot':'avg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig=plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.hist(authors.avg, log=True)\n",
    "plt.xlabel('Average # of Twitter Sent/Day')\n",
    "plt.ylabel('Counts')\n",
    "plt.title(r'Distribution of Twitters(Avg) Sent Every Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=pd.concat([italian_tweets[['content','author','time']],italian_tweets_old[['content','author','time']]])\n",
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prolific=authors.loc[authors.avg>20]\n",
    "tweets_prolific=tweets.copy().merge(prolific,how='right',on='author')\n",
    "tweets_prolific=tweets_prolific[tweets_prolific['time'].apply(lambda x:x>datetime.strptime('01-01-2017', '%d-%m-%Y'))]\n",
    "tweets_prolific.head()\n",
    "#tweets.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Italian politicians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=tweets[tweets['time'].apply(lambda x:x>datetime.strptime('01-01-2017', '%d-%m-%Y'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "renzi=tweets_prolific[tweets_prolific['content'].apply(lambda x: 'renzi' in x.lower())]\n",
    "renzi.content.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "renzi=tweets[tweets['content'].apply(lambda x: 'renzi' in x.lower())]\n",
    "renzi.content.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salvini=tweets_prolific[tweets_prolific['content'].apply(lambda x: 'salvini' in x.lower())]\n",
    "salvini.content.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salvini=tweets[tweets['content'].apply(lambda x: 'salvini' in x.lower())]\n",
    "#salvini=tweets.where(lambda x: 'salvini' in x.lower())\n",
    "salvini.content.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grillo=tweets[tweets['content'].apply(lambda x: 'grillo' in x.lower())]\n",
    "grillo.content.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep=tweets[tweets['content'].apply(lambda x: 'repubblica' in x.lower())]\n",
    "rep.content.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographical analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "munic = pd.read_excel('comuni.xls')\n",
    "munic=munic.rename(columns={'Denominazione in italiano':'comune','Denominazione regione':'regione'})[['comune','regione']]\n",
    "munic.regione=munic.regione.apply(lambda x: x.lower())\n",
    "munic['count']=0\n",
    "munic=munic.set_index('comune')\n",
    "munic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordInString(word, string_value):\n",
    "    return True if re.search(r'\\b' + word + r'\\b', string_value) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_c(name):\n",
    "    tweet_comune=tweets[tweets['content'].apply(lambda x: wordInString(name, x))]\n",
    "    total=tweet_comune.content.count()\n",
    "    munic.at[name, 'count']=total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_r(name):\n",
    "    tweet_reg=tweets[tweets['content'].apply(lambda x: wordInString(name.lower(), x.lower()))]\n",
    "    total=tweet_reg.content.count()\n",
    "    reg.at[name, 'tot']=reg.loc[name,'tot']+total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_comuni=munic.index.tolist()\n",
    "for m in list_comuni:\n",
    "    count_c(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=munic.groupby(by='regione')['count'].agg('sum').reset_index().rename(columns={'count':'tot'}).set_index('regione')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_regioni=reg.index.tolist()\n",
    "for m in list_regioni:\n",
    "    count_r(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times=pd.concat([italian_tweets[['content','author','time']],italian_tweets_old[['content','author','time']]])\n",
    "#times.loc[times['time'].apply(lambda x:x==datetime.strptime('26-03-2017', '%d-%m-%Y'))]\n",
    "times=times.drop_duplicates(subset='content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_tweet = times['time'].min()\n",
    "end_date_tweet = times['time'].max()\n",
    "print(start_date_tweet, end_date_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#times=pd.concat([italian_tweets[['content','author','time']],italian_tweets_old[['content','author','time']]])\n",
    "times['time']=times['time'].apply(lambda x: x.date())#authors.head()\n",
    "times['time'] = pd.to_datetime(times['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times=times.groupby(by='time',as_index=True).agg({'content':'count'}).reset_index().rename(columns={'content':'tot'})\n",
    "times.sort_values(by='time', ascending = True, inplace = True)\n",
    "times=times.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-darkgrid')\n",
    "times['tot'].plot(linestyle = \"-\", figsize = (12,8), rot = 45, color = 'k',\n",
    "                               linewidth = 1)\n",
    "plt.title('Tweet counts by date', fontsize = 15)\n",
    "plt.xlabel('Date', fontsize = 13)\n",
    "plt.ylabel('Tweet Count', fontsize = 13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak=pd.concat([italian_tweets[['content','author','time']],italian_tweets_old[['content','author','time']]])\n",
    "peak=peak.drop_duplicates(subset='content')\n",
    "peak=peak[peak['time'].apply(lambda x:\n",
    "                         ((x<datetime.strptime('01-02-2017', '%d-%m-%Y') or \n",
    "                          (x>datetime.strptime('31-05-2017', '%d-%m-%Y'))) & \n",
    "                          (x>datetime.strptime('01-03-2014', '%d-%m-%Y')) & \n",
    "                          (x<datetime.strptime('15-11-2017', '%d-%m-%Y')))\n",
    "                        )]\n",
    "peak['time']=peak['time'].apply(lambda x: x.date())#authors.head()\n",
    "peak['time'] = pd.to_datetime(peak['time'])\n",
    "peak=peak.groupby(by='time',as_index=True).agg({'content':'count'}).reset_index().rename(columns={'content':'tot'})\n",
    "peak.sort_values(by='time', ascending = True, inplace = True)\n",
    "peak=peak.set_index('time')\n",
    "\n",
    "dates_list = ['2016-12-01','2017-10-05']\n",
    "# 08-2017\n",
    "# March-May 2017 terror attacks in Europe: London, Stockholm, Paris, St Petersburg, Manchester\n",
    "# 12-2016 Italian referendum\n",
    "# 08-2015 Migratory crisis in Europe\n",
    "# 05-2014 European elections\n",
    "important_dates = pd.Series(pd.to_datetime(dates_list))\n",
    "\n",
    "summer = ['2017-07-15','2016-08-15','2015-08-10','2014-08-10']\n",
    "# 08-2017\n",
    "# March-May 2017 terror attacks in Europe: London, Stockholm, Paris, St Petersburg, Manchester\n",
    "# 12-2016 Italian referendum\n",
    "# 08-2015 Migratory crisis in Europe\n",
    "# 05-2014 European elections\n",
    "summer_dates = pd.Series(pd.to_datetime(summer))\n",
    "\n",
    "# add columns to identify important events, and mark a 0 or 1.\n",
    "peak['Summer Events'] = False\n",
    "peak.loc[summer_dates, 'Summer Events'] = True\n",
    "peak['vals'] = 0\n",
    "peak.loc[summer_dates, 'vals'] = 1\n",
    "\n",
    "peak['Important Events'] = False\n",
    "peak.loc[important_dates, 'Important Events'] = True\n",
    "peak['values'] = 0\n",
    "peak.loc[important_dates, 'values'] = 1\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "peak['tot'].plot(linestyle = \"-\", figsize = (22,8), rot = 45, color = 'k',\n",
    "                               linewidth = 1)\n",
    "plt.title('Tweet counts by date', fontsize = 15)\n",
    "plt.xlabel('Date', fontsize = 13)\n",
    "plt.ylabel('Tweet Count', fontsize = 13)\n",
    "\n",
    "plt.plot(peak[peak['Important Events'] == True].index.values,\n",
    "         peak.loc[peak['Important Events'] == True, 'values'],\n",
    "         marker = 'o', \n",
    "         color = 'r',\n",
    "         linestyle = 'none',\n",
    "        label = 'Important Dates in politics')\n",
    "\n",
    "plt.plot(peak[peak['Summer Events'] == True].index.values,\n",
    "         peak.loc[peak['Summer Events'] == True, 'vals'],\n",
    "         marker = 'o', \n",
    "         color = 'b',\n",
    "         linestyle = 'none',\n",
    "        label = 'Summer migratory crisis')\n",
    "\n",
    "for m in peak[peak['Summer Events'] == True].index.values:\n",
    "    plt.axvline(x=m)\n",
    "    \n",
    "for m in peak[peak['Important Events'] == True].index.values:\n",
    "    plt.axvline(x=m, color='r')\n",
    "\n",
    "plt.legend(loc = 'best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tweets = pd.read_csv('italian_tweets.csv')\n",
    "italian_tweets_old = pd.read_csv('italian_tweets_old_notusa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tweets['tweet_time'] =  pd.to_datetime(italian_tweets['tweet_time'], format='%Y/%m/%d %H:%M')\n",
    "italian_tweets_old['publish_date'] =  pd.to_datetime(italian_tweets_old['publish_date'], format='%m/%d/%Y %H:%M')\n",
    "italian_tweets=italian_tweets.rename(index=str, columns={\"user_display_name\":\"author\",\n",
    "                                                         \"follower_count_x\": \"followers\",\n",
    "                                                         \"following_count_x\": \"following\",\n",
    "                                                         \"tweet_time\":\"time\"})\n",
    "italian_tweets_old=italian_tweets_old.rename(index=str, columns={\"publish_date\":\"time\"})\n",
    "italian_tweets=italian_tweets.rename(index=str, columns={\"tweet_text\":\"content\"})\n",
    "ita=pd.concat([italian_tweets[['content','author','time']],italian_tweets_old[['content','author','time']]])\n",
    "ita=ita.drop_duplicates(subset='content')\n",
    "regex = re.compile('[^a-zA-Z ]')\n",
    "def returntest(text):\n",
    "    text=text.lower()\n",
    "    text=regex.sub('',text) \n",
    "    text=re.sub(r'(\\s)http\\w+', r' ', text)\n",
    "    return text\n",
    "new_text = ita.content.apply(returntest)\n",
    "print(new_text.head(3))\n",
    "new_text.to_csv(\"tweets_ita.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(rules, rules_to_show,strs):\n",
    "    G1 = nx.DiGraph()\n",
    "    color_map=[]\n",
    "    N = 50\n",
    "    colors = np.random.rand(N)       \n",
    "   \n",
    "    for i in range (rules_to_show):      \n",
    "        G1.add_nodes_from([\"R\"+str(i)])\n",
    "    \n",
    "        for a in rules.iloc[i]['antecedents']:\n",
    "            G1.add_nodes_from([a])\n",
    "            G1.add_edge(a, \"R\"+str(i), color=colors[i] , weight = 2)\n",
    "       \n",
    "        for c in rules.iloc[i]['consequents']:\n",
    "            G1.add_nodes_from([c])\n",
    "            G1.add_edge(\"R\"+str(i), c, color=colors[i],  weight=2)\n",
    "    \n",
    "    for node in G1:\n",
    "        found_a_string = False\n",
    "        \n",
    "        for item in strs: \n",
    "            if node==item:\n",
    "                found_a_string = True\n",
    "        if found_a_string:\n",
    "            color_map.append('yellow')\n",
    "        else:\n",
    "            color_map.append('red')       \n",
    " \n",
    " \n",
    "    edges = G1.edges()\n",
    "    colors = [G1[u][v]['color'] for u,v in edges]\n",
    "    weights = [G1[u][v]['weight'] for u,v in edges]\n",
    " \n",
    "    pos = nx.spring_layout(G1, k=16, scale=1)\n",
    "    nx.draw(G1, pos, edges=edges, node_color = color_map, edge_color=colors, width=weights, font_size=16, with_labels=False)            \n",
    "   \n",
    "    for p in pos:  # raise text positions\n",
    "        pos[p][1] += 0.07\n",
    "    \n",
    "    nx.draw_networkx_labels(G1, pos)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_tweets_text = pd.read_csv('tweets_ita.csv',header=None)\n",
    "print(italian_tweets_text.shape)\n",
    "stopwords = pd.read_json('stopwords-it.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_final = []\n",
    "for text in italian_tweets_text[1]:\n",
    "    list_final.append([word.lower() for word in text.split() \n",
    "                       if (word.lower() not in stopwords and\n",
    "                           (len(word)>3 or word in ['pd','ms']))])\n",
    "    \n",
    "list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(list_final).transform(list_final)\n",
    "df = pd.DataFrame(te_ary,columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_items = apriori(df, min_support=0.001, use_colnames=True, max_len = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couple = frequent_items [frequent_items['itemsets'].apply(lambda x: len(x)==2)]\n",
    "couple = couple.sort_values(by=['support'],ascending= False)\n",
    "couple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = association_rules(frequent_items, metric=\"confidence\", min_threshold=0.2)\n",
    "table = table.sort_values(by=['confidence'],ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = table[(table['antecedents'].apply(lambda x: 'pd' in str(x))) | (table['consequents'].apply(lambda x: 'pd' in str(x)))]\n",
    "strs=['R0', 'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'R9','R10', 'R11', 'R12', 'R13']\n",
    "draw_graph(pd, 14, strs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = table[(table['antecedents'].apply(lambda x: 'ms' in str(x))) | (table['consequents'].apply(lambda x: 'ms' in str(x)))]\n",
    "strs=['R0', 'R1', 'R2', 'R3']\n",
    "draw_graph(ms, 4, strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join([word for tweet in list_final for word in tweet])\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers=['Corriere della Sera','repubblica','repubblicait','SoleORE','QuotidianoNazionale',\n",
    "        'Gazzettait','laStampa','ilMessaggeroit','quotidianonazionale','ilpost',\n",
    "        'corriere','ilgiornale','Avvenire','Tuttosport','libero',\n",
    "        'FattoQuotidiano','Mattinodinapoli','huffpost','linkiesta',\n",
    "        'adnkronos','agenziaansa','video','giornalettismo','ilpost',\n",
    "        'lettera','huffpostitalia','agenziaitalia','skytg','skysport','internazionale',\n",
    "        'perch','foto','repubblicatv','wireditalia','news','diretta','leggo','radiorai']\n",
    "papers=[m.lower() for m in papers]\n",
    "text = \" \".join([word for tweet in list_final for word in tweet if word not in papers])\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
